{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Machine Learning</font>\n",
    "\n",
    "# <font color='blue'>Capítulo 15</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.8.2\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Projeto 7\n",
    "\n",
    "## Sistema de Recomendação de Filmes da Netflix\n",
    "\n",
    "![title](imagens/mini-projeto7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do Problema\n",
    "\n",
    "<p>\n",
    "O objetivo da Netflix é conectar as pessoas aos filmes que elas adoram. Para ajudar os clientes a encontrar esses filmes, eles desenvolveram um sistema de recomendação de filmes de classe mundial: CinematchSM. Seu trabalho é prever se alguém vai gostar de um filme com base no quanto gostou ou não de outros filmes. A Netflix usa essas previsões para fazer recomendações pessoais de filmes com base nos gostos exclusivos de cada cliente. E embora o <b> Cinematch </b> esteja indo muito bem, sempre pode ser melhorado.\n",
    "</p>\n",
    "<p> Existem várias abordagens alternativas interessantes de como o Cinematch funciona que o netflix ainda não experimentou. Alguns são descritos na literatura, outros não. Estamos curiosos para saber se algum deles pode vencer o Cinematch fazendo previsões melhores. Porque, francamente, se houver uma abordagem muito melhor, isso pode fazer uma grande diferença para nossos clientes e nossos negócios. </p>\n",
    "\n",
    "Objetivos:\n",
    "\n",
    "1. Prever a avaliação que um usuário daria a um filme que ainda não avaliou.\n",
    "2. Minimizar a diferença entre a avaliação prevista e real (RMSE e MAPE).\n",
    "\n",
    "Restrições:\n",
    "\n",
    "1. Alguma forma de interpretabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonte de Dados\n",
    "\n",
    "<p>\n",
    "A Netflix forneceu muitos dados de classificação anônimos e uma barra de precisão de predição que é 10% melhor do que o que o Cinematch pode fazer no mesmo conjunto de dados de treinamento. A precisão é uma medida de quão próximo as classificações previstas dos filmes correspondem às classificações reais subsequentes.\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "<li> <a href=\"https://www.netflixprize.com/rules.html\">Netflix Prize</a></li>\n",
    "<li> <a href=\"https://www.kaggle.com/netflix-inc/netflix-prize-data\">Dataset</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando e Carregando os Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Formatação dos gráficos\n",
    "matplotlib.use('nbagg')\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "matplotlib: 3.5.0\n",
      "seaborn   : 0.11.2\n",
      "scipy     : 1.6.2\n",
      "sklearn   : 1.0.2\n",
      "numpy     : 1.22.3\n",
      "pandas    : 1.3.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os Dados\n",
    "\n",
    "Para carregar os dados vamos executar as seguintes operações:\n",
    "\n",
    "- 1- Fazer a leitura das linhas de todos os arquivos disponíveis.\n",
    "- 2- Combinar todas as linhas de todos os arquivos em um único arquivo.\n",
    "- 3- Carregar o arquivo gerado em um dataframe do pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marca o início da execução de leitura dos arquivos.\n",
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo o arquivo dados/combined_data_1.txt...\n",
      "Concluído.\n",
      "\n",
      "Lendo o arquivo dados/combined_data_2.txt...\n",
      "Concluído.\n",
      "\n",
      "Lendo o arquivo dados/combined_data_3.txt...\n",
      "Concluído.\n",
      "\n",
      "Lendo o arquivo dados/combined_data_4.txt...\n",
      "Concluído.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criaremos um arquivo final chamado dados.csv\n",
    "\n",
    "# Se o arquivo não existir, criamos o arquivo em modo de escrita (w)\n",
    "if not os.path.isfile('dados/dados.csv'):\n",
    "    \n",
    "    # Cria e abre o arquivo para gravação\n",
    "    dataset = open('dados/dados.csv', mode = 'w')\n",
    "    \n",
    "    # Lista para as linhas dos arquivos\n",
    "    linhas = list()\n",
    "    \n",
    "    # Nomes e caminhos dos arquivos\n",
    "    arquivos = ['dados/combined_data_1.txt',\n",
    "                'dados/combined_data_2.txt', \n",
    "                'dados/combined_data_3.txt', \n",
    "                'dados/combined_data_4.txt']\n",
    "    \n",
    "    # Loop por cada arquivo na lista de arquivos\n",
    "    for arquivo in arquivos:\n",
    "        \n",
    "        # Print\n",
    "        print(\"Lendo o arquivo {}...\".format(arquivo))\n",
    "        \n",
    "        # Com o arquivo aberto, extraímos as linhas\n",
    "        with open(arquivo) as f:\n",
    "            \n",
    "            # Loop por cada linha do arquivo\n",
    "            for linha in f: \n",
    "                \n",
    "                # Deletamos o conteúdo da lista\n",
    "                del linhas[:] \n",
    "                \n",
    "                # Divide as linhas do arquivo pelo caracter de final de linha\n",
    "                linha = linha.strip()\n",
    "                \n",
    "                # Se encontramos \"dois pontos\" ao final da linha, fazemos replace removendo o caracter,\n",
    "                # pois queremos apenas o id do filme\n",
    "                if linha.endswith(':'):\n",
    "                    movie_id = linha.replace(':', '')\n",
    "                    \n",
    "                # Se não, criamos uma lista comprehension para fazer a separação das colunas por vírgula\n",
    "                else:\n",
    "                    \n",
    "                    # Separa as colunas\n",
    "                    linhas = [x for x in linha.split(',')]\n",
    "                    \n",
    "                    # Usa o id do filme na posição de índice zero\n",
    "                    linhas.insert(0, movie_id)\n",
    "                    \n",
    "                    # Grava o resultado no novo arquivo\n",
    "                    dataset.write(','.join(linhas))\n",
    "                    dataset.write('\\n')\n",
    "                    \n",
    "        print(\"Concluído.\\n\")\n",
    "        \n",
    "    dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo Total Para Carregar os Arquivos: 0:05:57.306483\n"
     ]
    }
   ],
   "source": [
    "# Imprime o tempop total\n",
    "print('Tempo Total Para Carregar os Arquivos:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando o dataframe pandas a partir do arquivo dados.csv...\n",
      "Concluído.\n",
      "CPU times: user 51 s, sys: 8.72 s, total: 59.8 s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Criando o dataframe pandas a partir do arquivo dados.csv...\")\n",
    "df_netflix = pd.read_csv('dados/dados.csv', sep = ',', names = ['movie', 'user', 'rating', 'date'])\n",
    "df_netflix.date = pd.to_datetime(df_netflix.date)\n",
    "print('Concluído.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordenando o dataframe por data..\n",
      "Concluído.\n",
      "CPU times: user 39.4 s, sys: 10.4 s, total: 49.8 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Ordenando o dataframe por data\n",
    "print('Ordenando o dataframe por data..')\n",
    "df_netflix.sort_values(by = 'date', inplace = True)\n",
    "print('Concluído.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100480507, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape\n",
    "df_netflix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56431994</th>\n",
       "      <td>10341</td>\n",
       "      <td>510180</td>\n",
       "      <td>4</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9056171</th>\n",
       "      <td>1798</td>\n",
       "      <td>510180</td>\n",
       "      <td>5</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58698779</th>\n",
       "      <td>10774</td>\n",
       "      <td>510180</td>\n",
       "      <td>3</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48101611</th>\n",
       "      <td>8651</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81893208</th>\n",
       "      <td>14660</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie    user  rating       date\n",
       "56431994  10341  510180       4 1999-11-11\n",
       "9056171    1798  510180       5 1999-11-11\n",
       "58698779  10774  510180       3 1999-11-11\n",
       "48101611   8651  510180       2 1999-11-11\n",
       "81893208  14660  510180       2 1999-11-11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizando os dados\n",
    "df_netflix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabalhar com 100 Milhões de Registros não é fácil e isso pode consumir muitos recursos computacionais. Algumas dicas:\n",
    "\n",
    "- 1- Feche todos os arquivos e softwares no seu computador. Deixe apenas o que for realmente necessário.\n",
    "- 2- Considere o uso de um ambiente em nuvem ou mesmo cluster de computadores, se possível.\n",
    "- 3- Reduza o tamanho de cada arquivo. Aqui algumas sugestões de softwares \"File Splitter\":\n",
    "\n",
    "http://www.fastfilejoiner.com/\n",
    "\n",
    "https://www.gdgsoft.com/gsplit/download\n",
    "\n",
    "http://www.kcsoftwares.com/?kfk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Exploratória dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo dos Dados\n",
      "--------------------------------------------------\n",
      "Número Total de Filmes: 17770\n",
      "Número Total de Usuários: 480189\n",
      "Número Total de Avaliações: 100480507\n"
     ]
    }
   ],
   "source": [
    "# Resumo dos dados\n",
    "print(\"Resumo dos Dados\")\n",
    "print(\"-\"*50)\n",
    "print(\"Número Total de Filmes:\", len(np.unique(df_netflix.movie)))\n",
    "print(\"Número Total de Usuários:\", len(np.unique(df_netflix.user)))\n",
    "print(\"Número Total de Avaliações:\", df_netflix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 s, sys: 259 ms, total: 16 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vamos salvar esses dois valores para usar mais tarde\n",
    "total_users = len(np.unique(df_netflix.user))\n",
    "total_movies = len(np.unique(df_netflix.movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.004805e+08\n",
       "mean     3.604290e+00\n",
       "std      1.085219e+00\n",
       "min      1.000000e+00\n",
       "25%      3.000000e+00\n",
       "50%      4.000000e+00\n",
       "75%      4.000000e+00\n",
       "max      5.000000e+00\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando a média das avaliações\n",
    "df_netflix.describe()['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando se temos valores ausentes\n",
    "sum(df_netflix.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando se temos valores duplicados (para esse caso não consideramos a data)\n",
    "sum(df_netflix.duplicated(['movie', 'user', 'rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dividir os dados em treino e teste antes de continuar como a análise exploratória, pois algumas análises só fazem sentido para os dados de treino. Usaremos a proporção 80/20 para treino/teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criaremos um dataset em disco com os dados de treino\n",
    "# Dessa forma não precisamos executar todo o processo de carga novamente cada vez que executar este notebook\n",
    "if not os.path.isfile('dados/dados_treino.csv'):\n",
    "    df_netflix.iloc[:int(df_netflix.shape[0] * 0.80)].to_csv(\"dados/dados_treino.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criaremos um dataset em disco com os dados de teste\n",
    "# Dessa forma não precisamos executar todo o processo de carga novamente cada vez que executar este notebook\n",
    "if not os.path.isfile('dados/dados_teste.csv'):\n",
    "    df_netflix.iloc[int(df_netflix.shape[0] * 0.80):].to_csv(\"dados/dados_teste.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletamos o dataframe original para liberar memória\n",
    "del df_netflix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando executar este Jupyter Notebook novamente, pode iniciar a partir desta célula abaixo (após carregar os pacotes).\n",
    "\n",
    "Caso tenha erro no Jupyter Noteboook, faça um refresh na aba com http://localhost:8888/tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora carregamos os arquivos em dataframes do pandas\n",
    "df_netflix_treino = pd.read_csv(\"dados/dados_treino.csv\", parse_dates = ['date'])\n",
    "df_netflix_teste = pd.read_csv(\"dados/dados_teste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo dos dados de treino\n",
    "print(\"Resumo dos Dados de Treino\")\n",
    "print(\"-\"*50)\n",
    "print(\"Número Total de Filmes:\", len(np.unique(df_netflix_treino.movie)))\n",
    "print(\"Número Total de Usuários:\", len(np.unique(df_netflix_treino.user)))\n",
    "print(\"Número Total de Avaliações:\", df_netflix_treino.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo dos dados de teste\n",
    "print(\"Resumo dos Dados de Teste\")\n",
    "print(\"-\"*50)\n",
    "print(\"Número Total de Filmes:\", len(np.unique(df_netflix_teste.movie)))\n",
    "print(\"Número Total de Usuários:\", len(np.unique(df_netflix_teste.user)))\n",
    "print(\"Número Total de Avaliações:\", df_netflix_teste.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função abaixo vai ajustar as medidas em milhares, milhões e bilhões para facilitar a leitura dos gráficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ajuste das unidades de medida\n",
    "def ajusta_unidades(num, units = 'M'):\n",
    "    units = units.lower()\n",
    "    num = float(num)\n",
    "    if units == 'k':\n",
    "        return str(num/10**3) + \" K\"\n",
    "    elif units == 'm':\n",
    "        return str(num/10**6) + \" M\"\n",
    "    elif units == 'b':\n",
    "        return str(num/10**9) +  \" B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress warnings\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vamos verificar a distribuição das avaliações.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Distribuição das Avaliaçõoes nos Dados de Treino', fontsize = 15)\n",
    "sns.countplot(df_netflix_treino.rating)\n",
    "ax.set_yticklabels([ajusta_unidades(item, 'M') for item in ax.get_yticks()])\n",
    "ax.set_ylabel('Número de Avaliações (em Milhões)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Será que o dia da semana tem influencia na avaliação do usuário? Vamos incluir uma coluna com o dia da semana e descobrir.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetro para evitar warning devido ao alto volume de dados\n",
    "pd.options.mode.chained_assignment = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai o dia da semana e grava em uma nova coluna\n",
    "df_netflix_treino['dia_semana'] = df_netflix_treino['date'].dt.strftime(\"%A\")\n",
    "df_netflix_treino.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "sns.countplot(x = 'dia_semana', data = df_netflix_treino, ax = ax)\n",
    "plt.title('Número de Avaliações Por Dia da Semana')\n",
    "plt.ylabel('Total de Avaliações')\n",
    "plt.xlabel('')\n",
    "ax.set_yticklabels([ajusta_unidades(item, 'M') for item in ax.get_yticks()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vamos calcular a média de avaliações por dia da semana.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Média de avaliações por dia da semana\n",
    "media_dia_semana = df_netflix_treino.groupby(by = ['dia_semana'])['rating'].mean()\n",
    "print(\"Média de Avaliações\")\n",
    "print(\"-\"*30)\n",
    "print(media_dia_semana)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dia da semana não parecer ter influência na avaliação dos usuários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vamos analisar as avaliações dos usuários ao longo do tempo.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig = plt.figure(figsize = plt.figaspect(.45))\n",
    "ax = df_netflix_treino.resample('m', on = 'date')['rating'].count().plot()\n",
    "ax.set_title('Número de Avaliações Por Mês nos Dados de Treino')\n",
    "plt.xlabel('Mês')\n",
    "plt.ylabel('Número de Avaliações Por Mês)')\n",
    "ax.set_yticklabels([ajusta_unidades(item, 'M') for item in ax.get_yticks()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente há um aumento nas avaliações dos usuários ao longo do tempo, devido ao maior número de usuários ou porque os usuários aprenderam a usar o recurso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vamos verificar os usuários que mais fizeram avaliações de filmes.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de avaliações por usuário\n",
    "num_aval_por_user = df_netflix_treino.groupby(by = 'user')['rating'].count().sort_values(ascending = False)\n",
    "num_aval_por_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo estatístico\n",
    "num_aval_por_user.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vamos criar um plot da função de função de densidade de probabilidade e da função de distribuição acumulada.__\n",
    "\n",
    "A função de densidade de probabilidade (pdf) e função de distribuição acumulada (cdf) são duas das funções estatísticas mais importantes em confiabilidade e estão intimamente relacionadas. Quando essas funções são conhecidas, quase qualquer outra medida de confiabilidade de interesse pode ser derivada ou obtida. Mais sobre isso aqui:\n",
    "\n",
    "http://reliawiki.org/index.php/Basic_Statistical_Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig = plt.figure(figsize = plt.figaspect(.45))\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(num_aval_por_user, shade = True, ax = ax1)\n",
    "plt.xlabel('Número de Avaliações Por Usuário')\n",
    "plt.title(\"PDF - Função de Densidade de Probabilidade\")\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(num_aval_por_user, shade = True, cumulative = True, ax = ax2)\n",
    "plt.xlabel('Número de Avaliações Por Usuário')\n",
    "plt.title('CDF - Função de Densidade Acumulada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que a grande maioria dos usuários tem menos de 1000 avaliações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quantas avaliações estão nos últimos 5% de todas as avaliações??__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos extrair os percentis\n",
    "percentis = num_aval_por_user.quantile(np.arange(0,1.01,0.01), interpolation = 'higher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando de 5 em 5\n",
    "percentis[::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig = plt.figure(figsize = plt.figaspect(.45))\n",
    "plt.title(\"Percentis\")\n",
    "percentis.plot()\n",
    "\n",
    "# Quartis com diferença de 0.05\n",
    "plt.scatter(x = percentis.index[::5], \n",
    "            y = percentis.values[::5], \n",
    "            c = 'orange', \n",
    "            label = \"Percentis Com Intervalos de 0.05\")\n",
    "\n",
    "# Quartis com diferença de 0.25\n",
    "plt.scatter(x = percentis.index[::25], \n",
    "            y = percentis.values[::25], \n",
    "            c = 'm', \n",
    "            label = \"Percentis Com Intervalos de 0.25\")\n",
    "\n",
    "# Labels e legenda\n",
    "plt.ylabel('Número de Avaliações Por Usuário')\n",
    "plt.xlabel('Valor no Percentis')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "# Vamos marcar os percentis 25, 50, 75 e 100\n",
    "for x,y in zip(percentis.index[::25], percentis[::25]):\n",
    "    plt.annotate(s = \"({} , {})\".format(x,y), xy = (x,y), xytext = (x-0.05, y+500), fontweight = 'bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Existem alguns filmes (que são muito populares) que são avaliados por um grande número de usuários.\n",
    "    \n",
    "- Mas a maioria dos filmes (como 90%) tem algumas centenas de avaliações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de Matriz Esparsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imagens/Sparse_Matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imagens/Matriz_Esparsa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação da Matriz Esparsa de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos a matriz esparsa no formato Numpy caso não exista\n",
    "# Se existir, apenas carregamos a partir do disco\n",
    "if os.path.isfile('dados/matriz_esparsa_treino.npz'):\n",
    "    matriz_esparsa_treino = sparse.load_npz('dados/matriz_esparsa_treino.npz')\n",
    "    print(\"Matriz Carregada.\")\n",
    "else: \n",
    "    matriz_esparsa_treino = sparse.csr_matrix((df_netflix_treino.rating.values, (df_netflix_treino.user.values, \n",
    "                                                                                 df_netflix_treino.movie.values)),)\n",
    "    print('Matriz Criada. O shape é: (user, movie): ', matriz_esparsa_treino.shape)\n",
    "    sparse.save_npz(\"dados/matriz_esparsa_treino.npz\", matriz_esparsa_treino)\n",
    "    print('Matriz Salva em Disco.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos a esparsidade da matriz\n",
    "linhas, colunas = matriz_esparsa_treino.shape\n",
    "elementos_nao_zero = matriz_esparsa_treino.count_nonzero()\n",
    "print(\"Esparsidade da Matriz de Treino: {} % \".format(  (1 - (elementos_nao_zero / (linhas * colunas))) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação da Matriz Esparsa de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos a matriz esparsa no formato Numpy caso não exista\n",
    "# Se existir, apenas carregamos a partir do disco\n",
    "if os.path.isfile('dados/matriz_esparsa_teste.npz'):\n",
    "    matriz_esparsa_teste = sparse.load_npz('dados/matriz_esparsa_teste.npz')\n",
    "    print(\"Matriz Carregada.\")\n",
    "else: \n",
    "    matriz_esparsa_teste = sparse.csr_matrix((df_netflix_teste.rating.values, (df_netflix_teste.user.values, \n",
    "                                                                               df_netflix_teste.movie.values)))\n",
    "    \n",
    "    print('Matriz Criada. O shape é: (user, movie): ', matriz_esparsa_teste.shape)\n",
    "    sparse.save_npz(\"dados/matriz_esparsa_teste.npz\", matriz_esparsa_teste)\n",
    "    print('Matriz Salva em Disco.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos a esparsidade da matriz\n",
    "linhas, colunas = matriz_esparsa_teste.shape\n",
    "elementos_nao_zero = matriz_esparsa_teste.count_nonzero()\n",
    "print(\"Esparsidade da Matriz de Teste: {} % \".format(  (1 - (elementos_nao_zero / (linhas * colunas))) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vamos calcular a média global de todas as avaliações de filmes, avaliação média por usuário e avaliação média por filme.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abaixo calculamos a média global de todas as avaliações de usuários.\n",
    "medias_treino = dict()\n",
    "medias_treino_global = matriz_esparsa_treino.sum() / matriz_esparsa_treino.count_nonzero()\n",
    "medias_treino['global'] = medias_treino_global\n",
    "medias_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vamos construir uma função para o cálculo da média de avaliações.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de cálculo da média\n",
    "def calcula_media_avaliacoes(sparse_matrix, of_users):\n",
    "    \n",
    "    # Média de avaliações de usuários/eixos\n",
    "    # 1 = eixo de usuário\n",
    "    # 0 = eixo de filme\n",
    "    ax = 1 if of_users else 0\n",
    "\n",
    "    # Soma\n",
    "    sum_of_ratings = sparse_matrix.sum(axis=ax).A1\n",
    "    \n",
    "    # Matriz booleana de avaliações ( se um usuário avaliou um filme ou não )\n",
    "    is_rated = sparse_matrix!=0\n",
    "    \n",
    "    # Número de avaliações de cada usuário ou filme\n",
    "    no_of_ratings = is_rated.sum(axis=ax).A1\n",
    "    \n",
    "    # Ids máximos de usuário e filmes na matriz esparsa\n",
    "    u, m = sparse_matrix.shape\n",
    "    \n",
    "    # Criamos um dicionário de usuários e suas avaliações médias.\n",
    "    media_aval = {i:sum_of_ratings[i]/no_of_ratings[i] for i in range(u if of_users else m) if no_of_ratings[i]!=0}\n",
    "\n",
    "    # Retorna o dicionário de médias de avaliações\n",
    "    return media_aval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Abaixo calculamos a média de avaliações por usuário.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Média de avaliações de usuários\n",
    "medias_treino['user'] = calcula_media_avaliacoes(matriz_esparsa_treino, of_users = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza o dicionário\n",
    "medias_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print\n",
    "print('Média de Avaliação do Usuário 149 :', medias_treino['user'][149])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Abaixo calculamos a média de avaliações por filme.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Média de avaliações por filme\n",
    "medias_treino['movie'] =  calcula_media_avaliacoes(matriz_esparsa_treino, of_users = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print\n",
    "print('Avaliação Média do Filme 32 :', medias_treino['movie'][32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PDFs e CDs da média. Avaliações de usuários e filmes (dados de treino).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = plt.figaspect(.45))\n",
    "fig.suptitle('Médias de Avaliações Por Usuário e Por Filme', fontsize = 15)\n",
    "\n",
    "ax1.set_title('Médias de Avaliações de Usuários')\n",
    "\n",
    "# Obtemos a lista de avaliações médias do usuário no dicionário de médias.\n",
    "medias_usuarios = [rat for rat in medias_treino['user'].values()]\n",
    "sns.distplot(medias_usuarios, ax = ax1, hist = False, kde_kws = dict(cumulative = True), label = 'CDF')\n",
    "sns.distplot(medias_usuarios, ax = ax1, hist = False, label = 'PDF')\n",
    "\n",
    "ax2.set_title('Médias de Avaliações de Filmes')\n",
    "\n",
    "# Obtemos a lista de avaliações médias de filmes do dicionário.\n",
    "medias_filmes = [rat for rat in medias_treino['movie'].values()]\n",
    "sns.distplot(medias_filmes, ax = ax2, hist = False, kde_kws = dict(cumulative = True), label = 'CDF')\n",
    "sns.distplot(medias_filmes, ax = ax2, hist = False, label = 'PDF')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema do Cold Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cold start de usários\n",
    "usuarios_treino = len(medias_treino['user'])\n",
    "novos_usuarios = total_users - usuarios_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print\n",
    "print('Total Geral de Usuários:', total_users)\n",
    "print('Total de Usuários em Treino :', usuarios_treino)\n",
    "print(\"Total de Usuários Que Não Estão em Treino: {} ({}%)\".format(novos_usuarios,\n",
    "                                                                   np.round((novos_usuarios / total_users) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75148 usuários não fazem parte dos dados de treino, ou seja, não temos como aprender o padrão de avaliação desses usuários! Esse é o problema do cold start (ou início frio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cold start de filmes\n",
    "filmes_treino = len(medias_treino['movie'])\n",
    "novos_filmes = total_movies - filmes_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print\n",
    "print('Total Geral de Filmes:', total_movies)\n",
    "print('Total de Filmes em Treino:', filmes_treino)\n",
    "print(\"Total de Filmes Que Não Estão em Treino: {} ({}%)\".format(novos_filmes,\n",
    "                                                                 np.round((novos_filmes/total_movies)*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "346 filmes não aparecem nos dados de treino. Teremos que lidar com isso quando trabalharmos especialmente no modelo de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a Matriz de Similaridade de Usuários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de cálculo de similaridade\n",
    "def calcula_similaridade_usuario(sparse_matrix, \n",
    "                                 compute_for_few = False, \n",
    "                                 top = 100, \n",
    "                                 verbose = False, \n",
    "                                 verb_for_n_rows = 20,\n",
    "                                 draw_time_taken = True):\n",
    "    \n",
    "    # Variáveis de controle\n",
    "    no_of_users, _ = sparse_matrix.shape\n",
    "    row_ind, col_ind = sparse_matrix.nonzero()\n",
    "    row_ind = sorted(set(row_ind)) \n",
    "    time_taken = list()\n",
    "    rows, cols, data = list(), list(), list()\n",
    "    if verbose: print(\"Calculando top\", top, \"similaridades para cada usuário...\")\n",
    "    start = datetime.now()\n",
    "    temp = 0\n",
    "    \n",
    "    # Loop pela matriz\n",
    "    for row in row_ind[:top] if compute_for_few else row_ind:\n",
    "        temp = temp + 1\n",
    "        prev = datetime.now()\n",
    "        \n",
    "        # Calculando a similaridade de cosseno\n",
    "        sim = cosine_similarity(sparse_matrix.getrow(row), sparse_matrix).ravel()\n",
    "        top_sim_ind = sim.argsort()[-top:]\n",
    "        top_sim_val = sim[top_sim_ind]\n",
    "        rows.extend([row]*top)\n",
    "        cols.extend(top_sim_ind)\n",
    "        data.extend(top_sim_val)\n",
    "        time_taken.append(datetime.now().timestamp() - prev.timestamp())\n",
    "        \n",
    "        if verbose:\n",
    "            if temp%verb_for_n_rows == 0:\n",
    "                print(\"Cálculo concluído para {} usuários [  tempo total : {}  ]\".format(temp, datetime.now()-start))\n",
    "            \n",
    "    if verbose: print('Criação de matriz esparsa a partir das semelhanças computadas...')    \n",
    "        \n",
    "    if draw_time_taken:\n",
    "        plt.plot(time_taken, label = 'Tempo de cálculo de cada usuário')\n",
    "        plt.plot(np.cumsum(time_taken), label = 'Tempo Total')\n",
    "        plt.legend(loc = 'best')\n",
    "        plt.xlabel('Usuário')\n",
    "        plt.ylabel('Tempo (segundos)')\n",
    "        plt.show()\n",
    "        \n",
    "    return sparse.csr_matrix((data, (rows, cols)), shape = (no_of_users, no_of_users)), time_taken      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos a similaridade\n",
    "\n",
    "# Marca o início\n",
    "start = datetime.now()\n",
    "\n",
    "# Calcula a similaridade\n",
    "matriz_esparsa_user, _ = calcula_similaridade_usuario(matriz_esparsa_treino, \n",
    "                                                      compute_for_few = True, \n",
    "                                                      top = 100, \n",
    "                                                      verbose = True)\n",
    "\n",
    "print(\"Tempo Total de Processamento:\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos **405.041 usuários** em nosso conjunto de treinamento e computação de semelhanças entre eles (**vetor dimensional de 17K**) é demorado.\n",
    "\n",
    "\n",
    "Tentaremos reduzir as dimensões usando SVD, de modo a acelerar o processo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redução de Dimensionalidade com TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redução de dimensionalidade\n",
    "\n",
    "# Marca o início\n",
    "start = datetime.now()\n",
    "\n",
    "# Cria o objeto TruncatedSVD reduzindo a dimensionalidade para 500 dimensões\n",
    "netflix_svd = TruncatedSVD(n_components = 500, algorithm = 'randomized', random_state = 15)\n",
    "\n",
    "# Aplica o TruncatedSVD\n",
    "trunc_svd = netflix_svd.fit_transform(matriz_esparsa_treino)\n",
    "\n",
    "print(\"Tempo Total de Processamento:\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vamos calcular a variância explicada pelos componentes.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a variância explicada\n",
    "expl_var = np.cumsum(netflix_svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, (ax1) = plt.subplots(nrows = 1, ncols = 1, figsize = plt.figaspect(.45))\n",
    "\n",
    "ax1.set_ylabel(\"Variância Explicada\", fontsize = 15)\n",
    "ax1.set_xlabel(\"Fatores Latentes\", fontsize = 15)\n",
    "ax1.plot(expl_var)\n",
    "\n",
    "# Vamos marcar algumas combinações de (fatores latentes, variância explicada) para tornar o gráfico mais claro\n",
    "ind = [1, 2, 4, 8, 20, 60, 100, 200, 300, 400, 500]\n",
    "ax1.scatter(x = [i-1 for i in ind], y = expl_var[[i-1 for i in ind]], c = '#ee4422')\n",
    "\n",
    "for i in ind:\n",
    "    ax1.annotate(s =\"({}, {})\".format(i,  np.round(expl_var[i-1], 2)), xy = (i-1, expl_var[i-1]),\n",
    "                xytext = ( i+20, expl_var[i-1] - 0.01), fontweight = 'bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com 500 componentes explicamos aproximadamente 65% da variância dos dados. Isso é suficiente para nosso exemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos projetar nossa matriz no espaço de 500 dimensões\n",
    "start = datetime.now()\n",
    "trunc_matrix = matriz_esparsa_treino.dot(netflix_svd.components_.T)\n",
    "print(\"Tempo de Processamento:\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "trunc_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo\n",
    "type(trunc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar e salvar em disco a matriz com a a dimensionalidade reduzida para 500 dimensões\n",
    "if not os.path.isfile('dados/matriz_esparsa_user_truncada.npz'):\n",
    "    matriz_esparsa_user_truncada = sparse.csr_matrix(trunc_matrix)\n",
    "    sparse.save_npz('dados/matriz_esparsa_user_truncada', matriz_esparsa_user_truncada)\n",
    "else:\n",
    "    matriz_esparsa_user_truncada = sparse.load_npz('dados/matriz_esparsa_user_truncada.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conferindo o shape\n",
    "matriz_esparsa_user_truncada.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Agora calculamos novamente a similaridade de usuários usando a matriz truncada.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula similaridade de usuários\n",
    "\n",
    "# Marca o início\n",
    "start = datetime.now()\n",
    "\n",
    "# Calcula a similaridade\n",
    "trunc_sim_matrix, _ = calcula_similaridade_usuario(matriz_esparsa_user_truncada, \n",
    "                                                   compute_for_few = True, \n",
    "                                                   top = 50, \n",
    "                                                   verbose = True) \n",
    "\n",
    "print(\"Tempo de Processamento:\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando Matriz de Similaridade de Filmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da similaridade de filmes\n",
    "\n",
    "# Marca o início\n",
    "start = datetime.now()\n",
    "\n",
    "# Cria se não existir\n",
    "if not os.path.isfile('dados/matriz_esparsa_filme.npz'):\n",
    "    matriz_esparsa_filme = cosine_similarity(X = matriz_esparsa_treino.T, dense_output = False)\n",
    "    print(\"Matriz Criada.\")\n",
    "    sparse.save_npz(\"dados/matriz_esparsa_filme.npz\", matriz_esparsa_filme)\n",
    "    print(\"Matriz Salva em Disco.\")\n",
    "else:\n",
    "    matriz_esparsa_filme = sparse.load_npz(\"dados/matriz_esparsa_filme.npz\")\n",
    "    print(\"Matriz Carregada.\")\n",
    "\n",
    "print(\"Tempo de Processamento:\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "matriz_esparsa_filme.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra os ids dos filmes\n",
    "movie_ids = np.unique(matriz_esparsa_filme.nonzero()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a similaridade de filmes de acordo com o padrão de avaliação dos usuários\n",
    "\n",
    "# Marca o início\n",
    "start = datetime.now()\n",
    "\n",
    "# Dicionário para armazenar as similaridades\n",
    "filmes_similares = dict()\n",
    "\n",
    "# Loop pelos ids dos filmes\n",
    "for movie in movie_ids:\n",
    "    # Obtemos os top filmes semelhantes e armazenamos no dicionário\n",
    "    filmes_sim = matriz_esparsa_filme[movie].toarray().ravel().argsort()[::-1][1:]\n",
    "    filmes_similares[movie] = filmes_sim[:100]\n",
    "    \n",
    "print(\"Tempo de Processamento:\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filmes similares ao filme de id 43\n",
    "filmes_similares[43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Agora vamos encontrar os filmes mais semelhantes usando a matriz de similaridade.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos carregar os títulos dos filmes do arquivo csv fornecido pela Netflix\n",
    "titulos_filmes = pd.read_csv(\"dados/movie_titles.csv\", \n",
    "                             sep = ',', \n",
    "                             header = None,\n",
    "                             names = ['ID_Filme', 'Ano_Lancamento', 'Titulo'], \n",
    "                             verbose = True,\n",
    "                             index_col = 'ID_Filme', \n",
    "                             encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza os dados\n",
    "titulos_filmes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vejamos quais são os filmes similares ao filme de ID 43.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID do filme\n",
    "id_filme = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print\n",
    "print(\"Filme:\", titulos_filmes.loc[id_filme].values[1])\n",
    "print(\"Total de Avaliações de Usuários = {}.\".format(matriz_esparsa_treino[:,id_filme].getnnz()))\n",
    "print(\"Encontramos {} filmes que são similares a este e vamos imprimir os mais similares.\".format(matriz_esparsa_filme[:,id_filme].getnnz()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrando todas as similaridades\n",
    "similarities = matriz_esparsa_filme[id_filme].toarray().ravel()\n",
    "similar_indices = similarities.argsort()[::-1][1:]\n",
    "similarities[similar_indices]\n",
    "sim_indices = similarities.argsort()[::-1][1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig = plt.figure(figsize = plt.figaspect(.45))\n",
    "plt.plot(similarities[sim_indices], label = 'Todas as Avaliações')\n",
    "plt.plot(similarities[sim_indices[:100]], label = 'Top 100 Filmes Similares')\n",
    "plt.title(\"Filmes Similares ao Filme {}\".format(id_filme), fontsize = 25)\n",
    "plt.xlabel(\"Filmes\", fontsize = 15)\n",
    "plt.ylabel(\"Similaridade de Cosseno\", fontsize = 15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui os top 10 filmes mais similares ao filme 43\n",
    "titulos_filmes.loc[sim_indices[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já poderíamos concluir o projeto aqui, pois já temos um sistema de recomendação. Mas iremos além e vamos construir um modelo de Machine Learning para fazer as previsões. Trabalharemos nisso na Parte 2 deste Mini-Projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
